# ğŸš€ ELT Pipeline with dbt + Snowflake + Airflow + Docker

A production-ready ELT (Extract, Load, Transform) pipeline built using modern data engineering tools.

---

## ğŸ§° Stack Used

| Tool         | Purpose                                             |
|--------------|-----------------------------------------------------|
| **Snowflake**| Cloud Data Warehouse for scalable storage & compute |
| **dbt**      | SQL-based data transformation + testing framework   |
| **Apache Airflow** | Workflow orchestration & automation         |
| **Docker**   | Containerization to ensure consistency              |

---

## âš™ï¸ Setup Instructions

### ğŸ§± 1. Clone the Repository
### ğŸ³ 2. Build Docker Environment
### â„ï¸ 3. Snowflake Setup
Create warehouse, role, schema, and user
Grant appropriate privileges
### ğŸ’» 4. Configure Airflow Connection
In Airflow UI:
Go to Admin > Connections
Add a new Snowflake connection:
Conn ID: snowflake_conn
Use JSON in the â€œExtraâ€ field:
### ğŸ§ª 5. Run dbt Models & Tests
### ğŸ“„ 6. Generate and View dbt Docs
### ğŸ“Š 7. DAG Visualization in Airflow
### ğŸ§  Features
- Clean separation of staging, intermediate, and fact models
- Built-in dbt tests for accuracy and data quality
- Reusable macros (like discounted_amount)
- Custom singular tests (like future dates or null revenue)
- Airflow DAG to schedule and orchestrate everything daily
- Modular, extensible, and ready for production pipelines

![Raw Data](https://github.com/user-attachments/assets/3eb69ede-7926-4ae6-bed2-1ae68462dc29)


## ğŸ‘‹ Author
#### Siddhant Mene
ğŸ“ Grad Student @ UT Dallas â€¢ ğŸ“Š Product & Data Enthusiast
ğŸ“« siddhant.mene@utdallas.edu
ğŸŒ LinkedIn: https://www.linkedin.com/in/siddhantmene/

